{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5810f0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464cc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=\"newdataset/train\"\n",
    "data_val=\"newdataset/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05adfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b29181",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=(128,128)\n",
    "input_shape=(128,128,3)\n",
    "num_classes=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e676ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb17bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),  # fixed typo here\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu', padding='same'),  # fixed typo here\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(6, activation='softmax')  # change 6 to your number of classes if needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  # use = not ==\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d903892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32768</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32768\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m4,194,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,289,350</span> (16.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,289,350\u001b[0m (16.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,288,902</span> (16.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,288,902\u001b[0m (16.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73978776",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "740931bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef03e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Project4-1\\\\newdataset\\\\newdataset\\\\train\"\n",
    "val_dir = \"C:\\\\Users\\\\hp\\\\OneDrive\\\\Desktop\\\\Project4-1\\\\newdataset\\\\newdataset\\\\val\"\n",
    "\n",
    "image_size = (128, 128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a556d728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train exists: True\n",
      "Val exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Train exists:\", os.path.exists(train_dir))\n",
    "print(\"Val exists:\", os.path.exists(val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62ae5be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\hp\\OneDrive\\Desktop\\Project4-1\n",
      "Files and folders here: ['main.ipynb', 'newdataset', 'newdataset.zip', 'project4.zip', 'transfer_model.h5']\n",
      "'newdataset' exists here: True\n",
      "Contents of 'newdataset': ['newdataset']\n",
      "'newdataset/train' exists: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Files and folders here:\", os.listdir())\n",
    "\n",
    "# Check if your 'newdataset' folder is here\n",
    "print(\"'newdataset' exists here:\", os.path.exists('newdataset'))\n",
    "\n",
    "# If it exists, list its contents\n",
    "if os.path.exists('newdataset'):\n",
    "    print(\"Contents of 'newdataset':\", os.listdir('newdataset'))\n",
    "\n",
    "#C:/Users/hp/neDrive/Desktop/Project4-1/newdataset/newdataset/train\n",
    "\n",
    "# Check if the train folder exists inside 'newdataset'\n",
    "print(\"'newdataset/train' exists:\", os.path.exists('C:/Users/hp/neDrive/Desktop/Project4-1/newdataset/newdataset/train'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "826f32b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2527 images belonging to 6 classes.\n",
      "Found 2527 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'C:/Users/hp/oneDrive/Desktop/Project4-1/newdataset/newdataset/train',\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    'C:/Users/hp/OneDrive/Desktop/Project4-1/newdataset/newdataset/val',\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1553fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4688c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <Variable path=sequential/conv2d/kernel, shape=(3, 3, 3, 32), dtype=float32, value=[[[[ 8.72995257e-02  1.05093203e-01  8.70281756e-02  1.14653692e-01\n    -7.49452189e-02 -3.03886123e-02 -5.55643290e-02 -3.19002606e-02\n    -1.28015175e-01  6.53397292e-02 -1.33641735e-01 -1.04116328e-01\n    -4.40943353e-02 -1.27690345e-01  9.71105248e-02 -8.17907751e-02\n     6.30734116e-02 -5.10560237e-02 -5.47280274e-02  1.82081610e-02\n     9.57127437e-02 -9.75694284e-02  1.31686136e-01 -7.22401887e-02\n    -8.41392428e-02  2.27732956e-02 -4.80802804e-02  8.97834897e-02\n    -2.41954848e-02  6.62396476e-02  2.19609383e-02 -5.60857914e-02]\n   [-5.69866002e-02 -1.00982435e-01 -5.33230305e-02 -4.67897765e-02\n     2.08923481e-02  2.13613957e-02 -5.82728572e-02  3.84314694e-02\n    -1.34872854e-01  8.02469477e-02 -5.79439066e-02  1.55453607e-01\n     9.02033821e-02 -1.92012951e-01 -1.01599999e-01  1.11807242e-01\n     1.03807196e-01 -3.13025862e-02 -3.28686610e-02 -5.32762669e-02\n     9.56891850e-02  2.67238519e-03  2.84935609e-02 -2.32441258e-02\n    -1.20083783e-02  1.31358787e-01 -1.23431183e-01 -1.55269161e-01\n    -2.04403587e-02  3.48045714e-02 -8.06315765e-02  4.80590649e-02]\n   [ 1.05592303e-01  1.02960944e-01 -7.74207860e-02  6.68351501e-02\n    -1.59460366e-01 -1.12216152e-01 -1.07713670e-01 -1.40815869e-01\n    -1.85165908e-02  1.10171311e-01 -4.14562412e-02  7.55314343e-03\n    -1.39641374e-01 -2.72631124e-02  1.33986399e-01 -3.24564129e-02\n    -1.25398114e-01 -2.98659094e-02  6.80398494e-02  9.00610983e-02\n     7.34625831e-02 -1.45209745e-01 -8.89736041e-02  1.12351254e-02\n    -1.09890677e-01 -3.28853242e-02  4.06719223e-02  1.93625141e-03\n    -1.00626066e-01  1.30563527e-01 -1.11703008e-01  1.00715667e-01]]\n\n  [[ 1.82194728e-02 -1.26683414e-01 -6.98459074e-02 -6.53997855e-03\n    -8.00964236e-02 -9.34787914e-02 -1.41281009e-01 -2.25532074e-02\n    -8.20081159e-02 -1.16926588e-01 -1.19392745e-01  5.42948246e-02\n    -9.33910161e-02 -5.82230128e-02 -8.15277640e-03  7.53843132e-03\n    -5.58195300e-02  1.05459817e-01 -1.49081554e-02  1.05506033e-01\n    -2.95420829e-02  1.31824121e-01 -4.15669382e-02 -2.52243374e-02\n     6.05216473e-02  1.94397345e-02 -1.08806863e-01  3.11907269e-02\n     4.77443337e-02 -5.49925305e-02 -1.41315848e-01  2.55723260e-02]\n   [-1.50665224e-01  4.40491140e-02  5.99024557e-02  1.36097163e-01\n     8.99950936e-02 -1.27353936e-01 -1.66099647e-03 -6.30378649e-02\n    -5.51413856e-02  4.26199213e-02  1.15482613e-01 -3.06892805e-02\n    -4.58711572e-02 -1.54821604e-01 -1.49746373e-01  3.92274335e-02\n    -1.05195142e-01 -3.18826400e-02 -8.08816031e-02  7.27886567e-03\n    -7.20017999e-02 -1.07302517e-01  4.70764413e-02 -1.02840468e-01\n     8.79035667e-02 -4.88287508e-02 -1.84065521e-01 -1.59698948e-01\n    -1.19335145e-01 -3.58177610e-02 -2.70042513e-02 -9.00674090e-02]\n   [-9.93037969e-02 -5.47767468e-02 -6.01525865e-02  1.11715369e-01\n    -4.36050780e-02 -1.12801448e-01 -3.57764289e-02  1.15744650e-01\n    -4.91671115e-02 -1.22980192e-01 -1.08463086e-01 -4.45235288e-03\n    -3.28892991e-02 -2.62587517e-02 -3.42511153e-03  1.90602988e-02\n    -9.70192254e-02  1.40580922e-01 -1.83494843e-03  9.42415148e-02\n    -1.53171182e-01  5.44470213e-02  6.63240254e-02  6.20025173e-02\n    -5.40323332e-02 -5.58258295e-02  8.38218480e-02  9.44379270e-02\n    -1.23440042e-01 -8.46943818e-03  1.18902102e-01  4.20380160e-02]]\n\n  [[-1.01487748e-01 -1.21486574e-01  1.44365758e-01 -1.24025777e-01\n    -7.41611645e-02  1.72055531e-02 -7.85141066e-03 -1.05830528e-01\n     1.24605812e-01 -8.90236646e-02  7.76140988e-02  6.12766109e-03\n    -8.28790888e-02  6.39648736e-02 -2.82060690e-02 -9.13895946e-03\n     1.12080961e-01  6.82314932e-02  5.15495278e-02 -1.09797083e-01\n     1.21027432e-01 -6.31517470e-02 -1.22820869e-01  1.61279351e-01\n    -4.01110835e-02 -2.76028719e-02  1.50908530e-02 -3.48467603e-02\n     3.47075760e-02 -1.37844637e-01 -1.53714940e-01 -1.07335240e-01]\n   [ 7.59973750e-02 -1.50109142e-01 -5.19049503e-02  1.32104471e-01\n    -9.23334137e-02 -1.47610828e-01 -1.28912106e-01  2.35879105e-02\n     1.59724906e-01 -2.50858702e-02 -1.33704826e-01  2.22365744e-02\n     3.18824165e-02 -1.16938896e-01  1.83573049e-02  4.81923623e-03\n     4.29122299e-02 -1.05893165e-01  1.14716284e-01  6.97090328e-02\n     2.92217415e-02 -2.38338001e-02 -6.29682885e-03  1.42599523e-01\n    -1.78658292e-01 -9.75127518e-02 -9.32088718e-02 -6.05773292e-02\n    -7.67338127e-02 -1.18686184e-01 -1.25311658e-01 -7.72634819e-02]\n   [ 2.75377296e-02  1.21026672e-01 -3.45999263e-02  8.16327110e-02\n    -6.67845877e-03 -1.59154788e-01  7.80742168e-02  8.90658721e-02\n     6.63770437e-02  5.98290823e-02  1.00473398e-02 -1.14799373e-01\n    -2.93200165e-02 -1.31108895e-01 -3.52209695e-02  1.28899992e-01\n     5.48223592e-02 -7.80979544e-02 -1.09964937e-01  5.85799757e-03\n     1.11369742e-02  1.23024113e-01  6.01240546e-02 -1.05178230e-01\n    -1.61398888e-01  7.43134245e-02  1.06549151e-01  9.11628306e-02\n     1.01731561e-01 -1.38238624e-01  1.16465501e-01  5.89952879e-02]]]\n\n\n [[[ 2.50664614e-02  7.31669590e-02 -1.00459024e-01  2.27536894e-02\n     5.43710329e-02  1.02781981e-01 -5.10681886e-03 -1.47663327e-02\n     1.12738825e-01 -7.11612627e-02  9.16944668e-02  1.29550695e-02\n    -1.23878740e-01 -8.58576875e-03 -1.21842651e-02 -1.12038953e-02\n    -4.63253260e-02  7.07516000e-02 -9.87568945e-02 -1.47807449e-01\n    -1.48277745e-01  1.79935452e-02  2.40599681e-02  1.27693824e-02\n     9.27173719e-02 -8.54431558e-03  1.24750383e-01 -2.07828786e-02\n     3.99814881e-02 -8.71300474e-02 -8.22961032e-02  4.41413708e-02]\n   [ 1.07756548e-01  2.18877252e-02 -4.06820551e-02 -2.98697203e-02\n     3.25852074e-02 -2.84489151e-02  1.53637603e-01 -5.86032197e-02\n    -1.05583139e-01 -6.56397492e-02  7.74984509e-02  3.49909440e-02\n    -7.79523775e-02 -8.10113251e-02 -6.69455677e-02  6.88322857e-02\n     4.64231009e-03  1.22412883e-01 -1.01659246e-01 -8.83838385e-02\n    -7.04352930e-02 -3.01771313e-02  1.02869019e-01  1.04115278e-01\n     7.11725429e-02  1.06674522e-01 -1.48675308e-01 -9.46521983e-02\n     5.94439320e-02 -5.76613098e-02  4.79167588e-02  8.54516476e-02]\n   [ 2.83558667e-02 -7.92413577e-02  6.72071725e-02  9.47236083e-03\n    -7.22198263e-02  1.29571065e-01 -1.85305290e-02  9.01728123e-02\n     9.70927924e-02 -1.11666657e-01  6.65396899e-02 -9.57381576e-02\n     5.50700314e-02 -8.32836237e-03 -8.22992474e-02  5.39412126e-02\n    -1.31057590e-01 -3.49133462e-02 -1.07551999e-01 -1.02698646e-01\n    -1.35004982e-01 -1.28375039e-01  1.47996508e-02  1.29075825e-01\n    -2.38429382e-02 -5.33662252e-02  3.26565802e-02  3.35350148e-02\n     5.94738089e-02 -1.13130957e-01  4.77364585e-02  7.99132511e-02]]\n\n  [[ 6.69666305e-02 -5.06920405e-02  6.29032329e-02 -1.34013419e-03\n    -8.66373628e-03 -5.03493436e-02 -3.21383402e-02 -1.48764312e-01\n     9.87597406e-02 -6.00193292e-02  6.57317489e-02  1.02342598e-01\n     8.12112540e-02  1.32834703e-01  2.74871271e-02  3.99049409e-02\n    -7.77124017e-02  1.07023761e-01  1.15668764e-02 -6.15204796e-02\n    -1.10340128e-02  1.27198547e-01 -1.21484045e-02  1.34394556e-01\n     4.09800224e-02 -3.46452706e-02 -3.91352996e-02  5.33533841e-02\n    -7.75558176e-03 -2.52638422e-02 -1.30429819e-01 -7.16087595e-02]\n   [ 1.03077084e-01 -5.86072244e-02 -1.29779011e-01  5.43714501e-02\n     8.91889930e-02  5.21270148e-02 -8.51331651e-02 -5.90297766e-02\n     1.92146283e-02  2.03953255e-02 -5.45426868e-02  4.53834534e-02\n     9.18653049e-03 -1.60647571e-01  3.20258364e-02  7.19170198e-02\n    -3.88918966e-02 -8.23938400e-02  1.35212213e-01 -1.60576627e-01\n     5.66455834e-02  5.02167493e-02  1.31135399e-03  1.18194163e-01\n    -5.02641574e-02  2.22416725e-02 -8.84887204e-02 -1.05264060e-01\n    -7.54916295e-02 -6.28433153e-02 -1.17162295e-01 -5.92406914e-02]\n   [-1.16607778e-01  1.59681421e-02  1.13391064e-01  5.48702292e-02\n     1.25156231e-02  3.75172161e-02  9.13577750e-02  2.30538733e-02\n    -4.29916196e-02 -1.35029182e-01 -1.82916578e-02 -3.32622454e-02\n    -1.25695094e-01  2.39869319e-02 -8.92354921e-02 -9.28371400e-02\n     3.98325250e-02  8.98685455e-02 -1.24289848e-01  1.00071663e-02\n     1.81096308e-02  1.44524917e-01 -9.97982547e-02 -5.68602979e-02\n    -7.96648953e-03  8.10671225e-02 -2.90775374e-02 -8.41234699e-02\n     4.30731922e-02 -1.12867124e-01 -2.87863277e-02  9.79799181e-02]]\n\n  [[-1.09195463e-01 -6.44494919e-03  7.85235539e-02 -3.25558335e-02\n     8.36543813e-02  3.06110177e-02 -5.50437458e-02  9.56175923e-02\n     9.73703936e-02  8.02486017e-02  4.30718549e-02  1.15027532e-01\n    -2.37915441e-02 -8.88809860e-02  4.82115634e-02  1.11318514e-01\n    -1.31205618e-01  3.12101524e-02  2.77976296e-03 -1.10923789e-01\n    -1.43582553e-01  1.22524060e-01  1.03798524e-01 -1.08771645e-01\n    -1.36177927e-01 -3.46112549e-02 -7.31856525e-02 -3.93404290e-02\n     1.35141909e-01 -9.65515003e-02 -9.69572067e-02  4.76219580e-02]\n   [-1.56398445e-01 -8.45207796e-02 -5.81939332e-02 -1.07465468e-01\n     2.62905098e-02  6.90333918e-02  1.26957685e-01  2.13020649e-02\n     1.56699106e-01 -6.64879009e-02 -8.28127488e-02  1.19106106e-01\n    -1.24626219e-01 -7.24881131e-04 -3.09811030e-02 -4.23418730e-02\n     8.96541327e-02 -8.23670477e-02 -4.34178896e-02  6.76673725e-02\n     2.75721923e-02 -6.77870065e-02  8.77423882e-02  8.95929113e-02\n     6.74035177e-02 -8.23280439e-02 -1.72086954e-01 -3.31522152e-02\n    -8.90908614e-02  7.58331865e-02  8.64314958e-02 -4.74633612e-02]\n   [-1.16361685e-01 -2.40869401e-03  4.52147648e-02 -3.72609533e-02\n     1.41252264e-01 -1.07219934e-01 -1.50584847e-01  1.24067932e-01\n     1.43600166e-01 -3.84484828e-02  1.48178115e-01 -1.10031649e-01\n     2.43738480e-02 -5.29302135e-02  1.05146229e-01 -1.23458534e-01\n    -4.06282023e-02  3.23283523e-02  3.15976739e-02  9.88605246e-02\n    -2.22668797e-02 -7.10277483e-02 -1.33676818e-02  8.56729075e-02\n    -1.04716895e-02 -1.42107373e-02  1.02247432e-01 -1.16730511e-01\n    -4.46531512e-02 -1.44067779e-01  2.94488878e-03  6.51923195e-02]]]\n\n\n [[[ 1.44703835e-01 -6.17521740e-02 -1.65461730e-02  8.71030390e-02\n     1.01278901e-01  8.84128809e-02  1.05857857e-01 -7.00827455e-03\n    -1.41305640e-01  2.14364454e-02 -6.29888475e-02 -7.15452433e-02\n    -1.54047891e-01  1.04855105e-01 -1.03136282e-02  5.72676174e-02\n    -2.22860873e-02  1.62892580e-01 -3.50332484e-02 -6.87520728e-02\n    -1.54243559e-01  9.02509168e-02 -1.15982167e-01  6.22127652e-02\n    -9.91552994e-02 -5.75277768e-02  4.82805334e-02  4.31873277e-02\n     1.46664783e-01 -1.13003418e-01 -1.52001902e-01 -1.18558787e-01]\n   [ 1.15862720e-01 -4.17611860e-02  8.31410587e-02  1.49167806e-01\n    -1.27685964e-01  1.31569549e-01  1.34836137e-01  4.65536751e-02\n    -1.04064927e-01 -1.73710175e-02  5.48995472e-02  6.29555359e-02\n     9.18746144e-02  3.74729037e-02 -1.67736840e-02 -1.08412080e-01\n     5.20833544e-02 -6.61857426e-03  3.55974063e-02 -1.41463410e-02\n    -3.29468921e-02 -4.73157950e-02 -1.00249864e-01 -2.53659058e-02\n    -9.38159134e-03 -1.19583979e-02  7.78153539e-02 -1.34426713e-01\n    -1.21494152e-01 -1.37519926e-01  9.08062011e-02 -9.17600617e-02]\n   [ 1.04092643e-01  6.30823895e-02  1.33149773e-01  6.49701357e-02\n    -2.56983843e-02 -1.28847346e-01  1.28349617e-01  1.21010132e-02\n    -5.40669262e-02 -1.03270851e-01  9.15057957e-02 -2.34079659e-02\n    -8.94256234e-02 -1.07883483e-01  3.50219868e-02 -2.91502289e-02\n     7.67068043e-02 -9.25639421e-02  8.27744603e-03 -3.16186771e-02\n    -1.28112752e-02 -4.23446409e-02 -1.01575732e-01 -3.17310318e-02\n     9.11605544e-03 -1.07420281e-01 -4.16727597e-03  1.23020168e-02\n    -2.63455734e-02  6.33188942e-03  1.24922149e-01  3.79477926e-02]]\n\n  [[ 7.14620575e-02  6.24160189e-03  6.59431294e-02 -5.61492071e-02\n     1.26596186e-02  6.04557730e-02  4.96350862e-02  4.42727506e-02\n    -6.95260465e-02 -7.17046112e-02  7.71753713e-02 -9.23098773e-02\n    -2.87756156e-02  6.46933466e-02 -3.66089754e-02  1.79025624e-02\n    -1.07974902e-01  4.29220386e-02 -6.53209537e-02 -2.88945418e-02\n     4.57594581e-02  4.05131765e-02 -8.12091753e-02  3.78039218e-02\n     1.17072895e-01  4.06682901e-02 -1.38250217e-02  9.70969629e-03\n     6.26590997e-02 -1.28996968e-01  5.86343892e-02 -1.42599806e-01]\n   [ 3.17348242e-02  8.29348341e-02  9.98957679e-02 -7.61895999e-02\n    -3.20134535e-02  1.22137360e-01  1.13704666e-01 -8.36730301e-02\n    -5.69566824e-02  3.17289717e-02  3.73967774e-02 -3.87309529e-02\n     1.07630782e-01 -1.39488950e-01 -1.96479936e-03 -1.59433618e-01\n    -7.66485259e-02 -7.32920840e-02 -1.19841605e-01 -8.71714279e-02\n    -1.57927513e-01 -1.22925267e-01  1.50868297e-02  6.71503693e-03\n     1.20318606e-01 -9.32964012e-02 -8.53434298e-03 -7.88278356e-02\n    -6.37809858e-02 -1.17676787e-01 -3.61556333e-04  1.37094175e-02]\n   [-8.28195084e-03 -1.31621230e-02  4.84061763e-02  1.25543237e-01\n    -1.31128654e-01  1.02287933e-01  1.36707583e-02  6.58677295e-02\n    -6.78677037e-02  6.13979027e-02  1.05126560e-01  3.53962742e-02\n     5.42316474e-02  1.13666572e-01 -5.21191619e-02 -1.36140555e-01\n    -1.69640750e-01 -7.54833370e-02 -1.43954335e-02  7.27354065e-02\n     7.51463547e-02  1.09645382e-01  7.67144412e-02  1.27040995e-02\n     4.49881107e-02  4.12841840e-03 -5.56196319e-03  4.84090373e-02\n     9.46924239e-02  5.34270294e-02 -2.43858118e-02  7.27111623e-02]]\n\n  [[-7.09167272e-02 -8.55607986e-02  7.65055269e-02  8.59899223e-02\n    -3.96864600e-02 -3.82562954e-04  7.11977705e-02 -5.67838624e-02\n    -2.73413211e-03 -9.57850643e-05  3.26001979e-02 -8.88872594e-02\n     1.76826008e-02  6.73860610e-02  3.80729288e-02 -1.16821997e-01\n     7.22190291e-02 -1.96704436e-02 -7.34387189e-02  1.02053106e-01\n     2.64811497e-02  3.01435702e-02  7.59642720e-02  1.45660520e-01\n    -1.07382394e-01 -1.05202690e-01  1.52519777e-01  8.62274040e-03\n    -8.79524723e-02 -1.18547723e-01 -1.48160890e-01 -2.64791958e-02]\n   [ 2.09985953e-02 -8.26038420e-02  6.59171715e-02  1.24216944e-01\n    -1.20021179e-01 -1.06440969e-01  7.56451190e-02 -8.31705853e-02\n    -4.45909463e-02 -8.64648968e-02 -1.20089553e-01  1.48757607e-01\n     1.16750322e-01  8.77752379e-02 -4.18109745e-02 -8.23961720e-02\n    -3.93401869e-02  2.48462781e-02 -1.29611358e-01 -1.45417070e-02\n     3.28931138e-02 -1.11635335e-01 -4.10660878e-02  3.66280111e-03\n     2.16146410e-02 -7.36377910e-02 -5.38553223e-02 -1.07759587e-01\n    -1.53541788e-01 -9.99205187e-02  5.22504263e-02 -1.22979529e-01]\n   [-6.50617257e-02 -1.28667921e-01  3.35596837e-02 -4.96796332e-02\n     1.26827434e-01 -1.00736566e-01  9.49570835e-02 -5.95847629e-02\n     7.21629187e-02 -2.32386682e-03  1.12317130e-01 -3.28679383e-02\n     4.14563641e-02 -6.82699829e-02 -2.46189926e-02  5.02638966e-02\n     3.23930457e-02 -6.79755285e-02 -7.91926607e-02 -1.19596973e-01\n    -8.87034461e-03  9.88648683e-02  1.59382727e-02  3.02823931e-02\n     1.02355368e-01  6.63700923e-02  6.52951375e-02 -3.72751728e-02\n     9.36363861e-02  1.63528211e-02 -5.06030582e-02  7.86745921e-02]]]]>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m\n\u001b[0;32m      1\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(\n\u001b[0;32m      2\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[0;32m     10\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m     12\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     13\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m history \u001b[38;5;241m=\u001b[39m cnn_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     17\u001b[0m     train_generator,\n\u001b[0;32m     18\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_generator),\n\u001b[0;32m     19\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[0;32m     20\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_generator,\n\u001b[0;32m     21\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_generator),\n\u001b[0;32m     22\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[reduce_lr, early_stopping]\u001b[38;5;66;03m#checkpoint\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     28\u001b[0m cnn_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransfer_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:290\u001b[0m, in \u001b[0;36mBaseOptimizer._check_variables_are_known\u001b[1;34m(self, variables)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables_indices:\n\u001b[1;32m--> 290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    291\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This optimizer can only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe called for the variables it was originally built with. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen working with a new set of variables, you should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecreate a new optimizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown variable: <Variable path=sequential/conv2d/kernel, shape=(3, 3, 3, 32), dtype=float32, value=[[[[ 8.72995257e-02  1.05093203e-01  8.70281756e-02  1.14653692e-01\n    -7.49452189e-02 -3.03886123e-02 -5.55643290e-02 -3.19002606e-02\n    -1.28015175e-01  6.53397292e-02 -1.33641735e-01 -1.04116328e-01\n    -4.40943353e-02 -1.27690345e-01  9.71105248e-02 -8.17907751e-02\n     6.30734116e-02 -5.10560237e-02 -5.47280274e-02  1.82081610e-02\n     9.57127437e-02 -9.75694284e-02  1.31686136e-01 -7.22401887e-02\n    -8.41392428e-02  2.27732956e-02 -4.80802804e-02  8.97834897e-02\n    -2.41954848e-02  6.62396476e-02  2.19609383e-02 -5.60857914e-02]\n   [-5.69866002e-02 -1.00982435e-01 -5.33230305e-02 -4.67897765e-02\n     2.08923481e-02  2.13613957e-02 -5.82728572e-02  3.84314694e-02\n    -1.34872854e-01  8.02469477e-02 -5.79439066e-02  1.55453607e-01\n     9.02033821e-02 -1.92012951e-01 -1.01599999e-01  1.11807242e-01\n     1.03807196e-01 -3.13025862e-02 -3.28686610e-02 -5.32762669e-02\n     9.56891850e-02  2.67238519e-03  2.84935609e-02 -2.32441258e-02\n    -1.20083783e-02  1.31358787e-01 -1.23431183e-01 -1.55269161e-01\n    -2.04403587e-02  3.48045714e-02 -8.06315765e-02  4.80590649e-02]\n   [ 1.05592303e-01  1.02960944e-01 -7.74207860e-02  6.68351501e-02\n    -1.59460366e-01 -1.12216152e-01 -1.07713670e-01 -1.40815869e-01\n    -1.85165908e-02  1.10171311e-01 -4.14562412e-02  7.55314343e-03\n    -1.39641374e-01 -2.72631124e-02  1.33986399e-01 -3.24564129e-02\n    -1.25398114e-01 -2.98659094e-02  6.80398494e-02  9.00610983e-02\n     7.34625831e-02 -1.45209745e-01 -8.89736041e-02  1.12351254e-02\n    -1.09890677e-01 -3.28853242e-02  4.06719223e-02  1.93625141e-03\n    -1.00626066e-01  1.30563527e-01 -1.11703008e-01  1.00715667e-01]]\n\n  [[ 1.82194728e-02 -1.26683414e-01 -6.98459074e-02 -6.53997855e-03\n    -8.00964236e-02 -9.34787914e-02 -1.41281009e-01 -2.25532074e-02\n    -8.20081159e-02 -1.16926588e-01 -1.19392745e-01  5.42948246e-02\n    -9.33910161e-02 -5.82230128e-02 -8.15277640e-03  7.53843132e-03\n    -5.58195300e-02  1.05459817e-01 -1.49081554e-02  1.05506033e-01\n    -2.95420829e-02  1.31824121e-01 -4.15669382e-02 -2.52243374e-02\n     6.05216473e-02  1.94397345e-02 -1.08806863e-01  3.11907269e-02\n     4.77443337e-02 -5.49925305e-02 -1.41315848e-01  2.55723260e-02]\n   [-1.50665224e-01  4.40491140e-02  5.99024557e-02  1.36097163e-01\n     8.99950936e-02 -1.27353936e-01 -1.66099647e-03 -6.30378649e-02\n    -5.51413856e-02  4.26199213e-02  1.15482613e-01 -3.06892805e-02\n    -4.58711572e-02 -1.54821604e-01 -1.49746373e-01  3.92274335e-02\n    -1.05195142e-01 -3.18826400e-02 -8.08816031e-02  7.27886567e-03\n    -7.20017999e-02 -1.07302517e-01  4.70764413e-02 -1.02840468e-01\n     8.79035667e-02 -4.88287508e-02 -1.84065521e-01 -1.59698948e-01\n    -1.19335145e-01 -3.58177610e-02 -2.70042513e-02 -9.00674090e-02]\n   [-9.93037969e-02 -5.47767468e-02 -6.01525865e-02  1.11715369e-01\n    -4.36050780e-02 -1.12801448e-01 -3.57764289e-02  1.15744650e-01\n    -4.91671115e-02 -1.22980192e-01 -1.08463086e-01 -4.45235288e-03\n    -3.28892991e-02 -2.62587517e-02 -3.42511153e-03  1.90602988e-02\n    -9.70192254e-02  1.40580922e-01 -1.83494843e-03  9.42415148e-02\n    -1.53171182e-01  5.44470213e-02  6.63240254e-02  6.20025173e-02\n    -5.40323332e-02 -5.58258295e-02  8.38218480e-02  9.44379270e-02\n    -1.23440042e-01 -8.46943818e-03  1.18902102e-01  4.20380160e-02]]\n\n  [[-1.01487748e-01 -1.21486574e-01  1.44365758e-01 -1.24025777e-01\n    -7.41611645e-02  1.72055531e-02 -7.85141066e-03 -1.05830528e-01\n     1.24605812e-01 -8.90236646e-02  7.76140988e-02  6.12766109e-03\n    -8.28790888e-02  6.39648736e-02 -2.82060690e-02 -9.13895946e-03\n     1.12080961e-01  6.82314932e-02  5.15495278e-02 -1.09797083e-01\n     1.21027432e-01 -6.31517470e-02 -1.22820869e-01  1.61279351e-01\n    -4.01110835e-02 -2.76028719e-02  1.50908530e-02 -3.48467603e-02\n     3.47075760e-02 -1.37844637e-01 -1.53714940e-01 -1.07335240e-01]\n   [ 7.59973750e-02 -1.50109142e-01 -5.19049503e-02  1.32104471e-01\n    -9.23334137e-02 -1.47610828e-01 -1.28912106e-01  2.35879105e-02\n     1.59724906e-01 -2.50858702e-02 -1.33704826e-01  2.22365744e-02\n     3.18824165e-02 -1.16938896e-01  1.83573049e-02  4.81923623e-03\n     4.29122299e-02 -1.05893165e-01  1.14716284e-01  6.97090328e-02\n     2.92217415e-02 -2.38338001e-02 -6.29682885e-03  1.42599523e-01\n    -1.78658292e-01 -9.75127518e-02 -9.32088718e-02 -6.05773292e-02\n    -7.67338127e-02 -1.18686184e-01 -1.25311658e-01 -7.72634819e-02]\n   [ 2.75377296e-02  1.21026672e-01 -3.45999263e-02  8.16327110e-02\n    -6.67845877e-03 -1.59154788e-01  7.80742168e-02  8.90658721e-02\n     6.63770437e-02  5.98290823e-02  1.00473398e-02 -1.14799373e-01\n    -2.93200165e-02 -1.31108895e-01 -3.52209695e-02  1.28899992e-01\n     5.48223592e-02 -7.80979544e-02 -1.09964937e-01  5.85799757e-03\n     1.11369742e-02  1.23024113e-01  6.01240546e-02 -1.05178230e-01\n    -1.61398888e-01  7.43134245e-02  1.06549151e-01  9.11628306e-02\n     1.01731561e-01 -1.38238624e-01  1.16465501e-01  5.89952879e-02]]]\n\n\n [[[ 2.50664614e-02  7.31669590e-02 -1.00459024e-01  2.27536894e-02\n     5.43710329e-02  1.02781981e-01 -5.10681886e-03 -1.47663327e-02\n     1.12738825e-01 -7.11612627e-02  9.16944668e-02  1.29550695e-02\n    -1.23878740e-01 -8.58576875e-03 -1.21842651e-02 -1.12038953e-02\n    -4.63253260e-02  7.07516000e-02 -9.87568945e-02 -1.47807449e-01\n    -1.48277745e-01  1.79935452e-02  2.40599681e-02  1.27693824e-02\n     9.27173719e-02 -8.54431558e-03  1.24750383e-01 -2.07828786e-02\n     3.99814881e-02 -8.71300474e-02 -8.22961032e-02  4.41413708e-02]\n   [ 1.07756548e-01  2.18877252e-02 -4.06820551e-02 -2.98697203e-02\n     3.25852074e-02 -2.84489151e-02  1.53637603e-01 -5.86032197e-02\n    -1.05583139e-01 -6.56397492e-02  7.74984509e-02  3.49909440e-02\n    -7.79523775e-02 -8.10113251e-02 -6.69455677e-02  6.88322857e-02\n     4.64231009e-03  1.22412883e-01 -1.01659246e-01 -8.83838385e-02\n    -7.04352930e-02 -3.01771313e-02  1.02869019e-01  1.04115278e-01\n     7.11725429e-02  1.06674522e-01 -1.48675308e-01 -9.46521983e-02\n     5.94439320e-02 -5.76613098e-02  4.79167588e-02  8.54516476e-02]\n   [ 2.83558667e-02 -7.92413577e-02  6.72071725e-02  9.47236083e-03\n    -7.22198263e-02  1.29571065e-01 -1.85305290e-02  9.01728123e-02\n     9.70927924e-02 -1.11666657e-01  6.65396899e-02 -9.57381576e-02\n     5.50700314e-02 -8.32836237e-03 -8.22992474e-02  5.39412126e-02\n    -1.31057590e-01 -3.49133462e-02 -1.07551999e-01 -1.02698646e-01\n    -1.35004982e-01 -1.28375039e-01  1.47996508e-02  1.29075825e-01\n    -2.38429382e-02 -5.33662252e-02  3.26565802e-02  3.35350148e-02\n     5.94738089e-02 -1.13130957e-01  4.77364585e-02  7.99132511e-02]]\n\n  [[ 6.69666305e-02 -5.06920405e-02  6.29032329e-02 -1.34013419e-03\n    -8.66373628e-03 -5.03493436e-02 -3.21383402e-02 -1.48764312e-01\n     9.87597406e-02 -6.00193292e-02  6.57317489e-02  1.02342598e-01\n     8.12112540e-02  1.32834703e-01  2.74871271e-02  3.99049409e-02\n    -7.77124017e-02  1.07023761e-01  1.15668764e-02 -6.15204796e-02\n    -1.10340128e-02  1.27198547e-01 -1.21484045e-02  1.34394556e-01\n     4.09800224e-02 -3.46452706e-02 -3.91352996e-02  5.33533841e-02\n    -7.75558176e-03 -2.52638422e-02 -1.30429819e-01 -7.16087595e-02]\n   [ 1.03077084e-01 -5.86072244e-02 -1.29779011e-01  5.43714501e-02\n     8.91889930e-02  5.21270148e-02 -8.51331651e-02 -5.90297766e-02\n     1.92146283e-02  2.03953255e-02 -5.45426868e-02  4.53834534e-02\n     9.18653049e-03 -1.60647571e-01  3.20258364e-02  7.19170198e-02\n    -3.88918966e-02 -8.23938400e-02  1.35212213e-01 -1.60576627e-01\n     5.66455834e-02  5.02167493e-02  1.31135399e-03  1.18194163e-01\n    -5.02641574e-02  2.22416725e-02 -8.84887204e-02 -1.05264060e-01\n    -7.54916295e-02 -6.28433153e-02 -1.17162295e-01 -5.92406914e-02]\n   [-1.16607778e-01  1.59681421e-02  1.13391064e-01  5.48702292e-02\n     1.25156231e-02  3.75172161e-02  9.13577750e-02  2.30538733e-02\n    -4.29916196e-02 -1.35029182e-01 -1.82916578e-02 -3.32622454e-02\n    -1.25695094e-01  2.39869319e-02 -8.92354921e-02 -9.28371400e-02\n     3.98325250e-02  8.98685455e-02 -1.24289848e-01  1.00071663e-02\n     1.81096308e-02  1.44524917e-01 -9.97982547e-02 -5.68602979e-02\n    -7.96648953e-03  8.10671225e-02 -2.90775374e-02 -8.41234699e-02\n     4.30731922e-02 -1.12867124e-01 -2.87863277e-02  9.79799181e-02]]\n\n  [[-1.09195463e-01 -6.44494919e-03  7.85235539e-02 -3.25558335e-02\n     8.36543813e-02  3.06110177e-02 -5.50437458e-02  9.56175923e-02\n     9.73703936e-02  8.02486017e-02  4.30718549e-02  1.15027532e-01\n    -2.37915441e-02 -8.88809860e-02  4.82115634e-02  1.11318514e-01\n    -1.31205618e-01  3.12101524e-02  2.77976296e-03 -1.10923789e-01\n    -1.43582553e-01  1.22524060e-01  1.03798524e-01 -1.08771645e-01\n    -1.36177927e-01 -3.46112549e-02 -7.31856525e-02 -3.93404290e-02\n     1.35141909e-01 -9.65515003e-02 -9.69572067e-02  4.76219580e-02]\n   [-1.56398445e-01 -8.45207796e-02 -5.81939332e-02 -1.07465468e-01\n     2.62905098e-02  6.90333918e-02  1.26957685e-01  2.13020649e-02\n     1.56699106e-01 -6.64879009e-02 -8.28127488e-02  1.19106106e-01\n    -1.24626219e-01 -7.24881131e-04 -3.09811030e-02 -4.23418730e-02\n     8.96541327e-02 -8.23670477e-02 -4.34178896e-02  6.76673725e-02\n     2.75721923e-02 -6.77870065e-02  8.77423882e-02  8.95929113e-02\n     6.74035177e-02 -8.23280439e-02 -1.72086954e-01 -3.31522152e-02\n    -8.90908614e-02  7.58331865e-02  8.64314958e-02 -4.74633612e-02]\n   [-1.16361685e-01 -2.40869401e-03  4.52147648e-02 -3.72609533e-02\n     1.41252264e-01 -1.07219934e-01 -1.50584847e-01  1.24067932e-01\n     1.43600166e-01 -3.84484828e-02  1.48178115e-01 -1.10031649e-01\n     2.43738480e-02 -5.29302135e-02  1.05146229e-01 -1.23458534e-01\n    -4.06282023e-02  3.23283523e-02  3.15976739e-02  9.88605246e-02\n    -2.22668797e-02 -7.10277483e-02 -1.33676818e-02  8.56729075e-02\n    -1.04716895e-02 -1.42107373e-02  1.02247432e-01 -1.16730511e-01\n    -4.46531512e-02 -1.44067779e-01  2.94488878e-03  6.51923195e-02]]]\n\n\n [[[ 1.44703835e-01 -6.17521740e-02 -1.65461730e-02  8.71030390e-02\n     1.01278901e-01  8.84128809e-02  1.05857857e-01 -7.00827455e-03\n    -1.41305640e-01  2.14364454e-02 -6.29888475e-02 -7.15452433e-02\n    -1.54047891e-01  1.04855105e-01 -1.03136282e-02  5.72676174e-02\n    -2.22860873e-02  1.62892580e-01 -3.50332484e-02 -6.87520728e-02\n    -1.54243559e-01  9.02509168e-02 -1.15982167e-01  6.22127652e-02\n    -9.91552994e-02 -5.75277768e-02  4.82805334e-02  4.31873277e-02\n     1.46664783e-01 -1.13003418e-01 -1.52001902e-01 -1.18558787e-01]\n   [ 1.15862720e-01 -4.17611860e-02  8.31410587e-02  1.49167806e-01\n    -1.27685964e-01  1.31569549e-01  1.34836137e-01  4.65536751e-02\n    -1.04064927e-01 -1.73710175e-02  5.48995472e-02  6.29555359e-02\n     9.18746144e-02  3.74729037e-02 -1.67736840e-02 -1.08412080e-01\n     5.20833544e-02 -6.61857426e-03  3.55974063e-02 -1.41463410e-02\n    -3.29468921e-02 -4.73157950e-02 -1.00249864e-01 -2.53659058e-02\n    -9.38159134e-03 -1.19583979e-02  7.78153539e-02 -1.34426713e-01\n    -1.21494152e-01 -1.37519926e-01  9.08062011e-02 -9.17600617e-02]\n   [ 1.04092643e-01  6.30823895e-02  1.33149773e-01  6.49701357e-02\n    -2.56983843e-02 -1.28847346e-01  1.28349617e-01  1.21010132e-02\n    -5.40669262e-02 -1.03270851e-01  9.15057957e-02 -2.34079659e-02\n    -8.94256234e-02 -1.07883483e-01  3.50219868e-02 -2.91502289e-02\n     7.67068043e-02 -9.25639421e-02  8.27744603e-03 -3.16186771e-02\n    -1.28112752e-02 -4.23446409e-02 -1.01575732e-01 -3.17310318e-02\n     9.11605544e-03 -1.07420281e-01 -4.16727597e-03  1.23020168e-02\n    -2.63455734e-02  6.33188942e-03  1.24922149e-01  3.79477926e-02]]\n\n  [[ 7.14620575e-02  6.24160189e-03  6.59431294e-02 -5.61492071e-02\n     1.26596186e-02  6.04557730e-02  4.96350862e-02  4.42727506e-02\n    -6.95260465e-02 -7.17046112e-02  7.71753713e-02 -9.23098773e-02\n    -2.87756156e-02  6.46933466e-02 -3.66089754e-02  1.79025624e-02\n    -1.07974902e-01  4.29220386e-02 -6.53209537e-02 -2.88945418e-02\n     4.57594581e-02  4.05131765e-02 -8.12091753e-02  3.78039218e-02\n     1.17072895e-01  4.06682901e-02 -1.38250217e-02  9.70969629e-03\n     6.26590997e-02 -1.28996968e-01  5.86343892e-02 -1.42599806e-01]\n   [ 3.17348242e-02  8.29348341e-02  9.98957679e-02 -7.61895999e-02\n    -3.20134535e-02  1.22137360e-01  1.13704666e-01 -8.36730301e-02\n    -5.69566824e-02  3.17289717e-02  3.73967774e-02 -3.87309529e-02\n     1.07630782e-01 -1.39488950e-01 -1.96479936e-03 -1.59433618e-01\n    -7.66485259e-02 -7.32920840e-02 -1.19841605e-01 -8.71714279e-02\n    -1.57927513e-01 -1.22925267e-01  1.50868297e-02  6.71503693e-03\n     1.20318606e-01 -9.32964012e-02 -8.53434298e-03 -7.88278356e-02\n    -6.37809858e-02 -1.17676787e-01 -3.61556333e-04  1.37094175e-02]\n   [-8.28195084e-03 -1.31621230e-02  4.84061763e-02  1.25543237e-01\n    -1.31128654e-01  1.02287933e-01  1.36707583e-02  6.58677295e-02\n    -6.78677037e-02  6.13979027e-02  1.05126560e-01  3.53962742e-02\n     5.42316474e-02  1.13666572e-01 -5.21191619e-02 -1.36140555e-01\n    -1.69640750e-01 -7.54833370e-02 -1.43954335e-02  7.27354065e-02\n     7.51463547e-02  1.09645382e-01  7.67144412e-02  1.27040995e-02\n     4.49881107e-02  4.12841840e-03 -5.56196319e-03  4.84090373e-02\n     9.46924239e-02  5.34270294e-02 -2.43858118e-02  7.27111623e-02]]\n\n  [[-7.09167272e-02 -8.55607986e-02  7.65055269e-02  8.59899223e-02\n    -3.96864600e-02 -3.82562954e-04  7.11977705e-02 -5.67838624e-02\n    -2.73413211e-03 -9.57850643e-05  3.26001979e-02 -8.88872594e-02\n     1.76826008e-02  6.73860610e-02  3.80729288e-02 -1.16821997e-01\n     7.22190291e-02 -1.96704436e-02 -7.34387189e-02  1.02053106e-01\n     2.64811497e-02  3.01435702e-02  7.59642720e-02  1.45660520e-01\n    -1.07382394e-01 -1.05202690e-01  1.52519777e-01  8.62274040e-03\n    -8.79524723e-02 -1.18547723e-01 -1.48160890e-01 -2.64791958e-02]\n   [ 2.09985953e-02 -8.26038420e-02  6.59171715e-02  1.24216944e-01\n    -1.20021179e-01 -1.06440969e-01  7.56451190e-02 -8.31705853e-02\n    -4.45909463e-02 -8.64648968e-02 -1.20089553e-01  1.48757607e-01\n     1.16750322e-01  8.77752379e-02 -4.18109745e-02 -8.23961720e-02\n    -3.93401869e-02  2.48462781e-02 -1.29611358e-01 -1.45417070e-02\n     3.28931138e-02 -1.11635335e-01 -4.10660878e-02  3.66280111e-03\n     2.16146410e-02 -7.36377910e-02 -5.38553223e-02 -1.07759587e-01\n    -1.53541788e-01 -9.99205187e-02  5.22504263e-02 -1.22979529e-01]\n   [-6.50617257e-02 -1.28667921e-01  3.35596837e-02 -4.96796332e-02\n     1.26827434e-01 -1.00736566e-01  9.49570835e-02 -5.95847629e-02\n     7.21629187e-02 -2.32386682e-03  1.12317130e-01 -3.28679383e-02\n     4.14563641e-02 -6.82699829e-02 -2.46189926e-02  5.02638966e-02\n     3.23930457e-02 -6.79755285e-02 -7.91926607e-02 -1.19596973e-01\n    -8.87034461e-03  9.88648683e-02  1.59382727e-02  3.02823931e-02\n     1.02355368e-01  6.63700923e-02  6.52951375e-02 -3.72751728e-02\n     9.36363861e-02  1.63528211e-02 -5.06030582e-02  7.86745921e-02]]]]>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=25,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator),\n",
    "    callbacks=[reduce_lr, early_stopping]#checkpoint\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "cnn_model.save('transfer_model.h5')\n",
    "\n",
    "model_path = \"transfer_model.h5\" \n",
    " \n",
    "model_size = os.path.getsize(model_path)\n",
    " \n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "model_path = \"transfer_model.h5\"\n",
    "cnn_model = load_model(model_path)\n",
    "\n",
    "model_size = os.path.getsize(model_path)\n",
    "model_size_mb = model_size / (1024 * 1024)\n",
    "print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "img_path = \"cardboard_391.jpg\"\n",
    "\n",
    "img = image.load_img(img_path, target_size=(128, 128))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = img_array / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "pred = cnn_model.predict(img_array)\n",
    "predicted_class = np.argmax(pred, axis=1)[0]\n",
    "\n",
    "class_labels = ['cardboard', 'glass', 'metal', 'paper','plastic','trash'] \n",
    "predicted_label = class_labels[predicted_class]\n",
    "\n",
    "print(f\"Predicted Class Index: {predicted_class}\")\n",
    "print(f\"Predicted Class Label: {predicted_label}\")\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f67ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4fa023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
